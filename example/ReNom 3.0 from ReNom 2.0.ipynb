{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import renom as rm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReNom v3.0 for v2.0+ users\n",
    "\n",
    "For users familiar with the v2.0 framework, this example gives several short side-by-side comparisons of v2.0 and v3.0. The two main purposes of v3.0 is to take many of the functions that previously were loosely related before and insert them into a single form that reduces overhead of jumping between the different modes, allows for more optimization and better support for multiple devices. \n",
    "In short, v2.0 worked by gathering the \"history\" of operations being performed and using this history as a way to determine what calculations to perform. ReNom was in other words dependent on the user constantly rebuilding and tearing down the history in order to produce its calculations.\n",
    "\n",
    "v3.0 on the other hand, introduces the idea of a calculation graph, allowing ReNom to predict what the user wants to do, so long as it is told what the graph should look like at least once. Having control over the graph and being able to observe inputs and outputs coming to and from ReNom is for debugging purposes very beneficial, so it is still possible to use v3.0 in the same way that v2.0 was used, but we now allow for a different, more light-weight and optimized way of running ReNom.\n",
    "\n",
    "One of the key goals of the ReNom v3.0 implementation was that previous users of ReNom should not have to adapt much in order to update their code to use the new multi-gpu oriented features. For most of the layers, this means that all that has to change is the name and location of the operations being executed. As an example, a fully-connected layer is not constructed using rm.Dense, but should be constructed using rm.graph.DenseGraphElement instead. The new naming convention is a way to make sure that users are aware of the change from v2.0 to v3.0 by 'opting-in' using the new names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some basic examples\n",
    "\n",
    "What follows next is a side-by-side comparison of v2.0 and v3.0. In constructing the graph, there are made very few changes in how to perform the forward calculations, which is as described above on purpose.\n",
    "\n",
    "For a simple Dense network, v2.0 uses the name __Dense__, whereas v3.0 requires the use __graph.DenseGraphElement__. Both accept NumPy arrays as inputs and can the results of performing the operations can be printed to the python interpreter immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old result:\n",
      "[[ 1.  1.  1.]\n",
      " [ 5.  5.  5.]]\n",
      "New result:\n",
      "[[ 1.  1.  1.]\n",
      " [ 5.  5.  5.]]\n"
     ]
    }
   ],
   "source": [
    "arr = np.arange(4).reshape(2,2)\n",
    "init = rm.utility.initializer.Constant(1)\n",
    "\n",
    "\n",
    "model_old1 = rm.Dense(3, initializer = init)\n",
    "model_new1 = rm.graph.DenseGraphElement(3, initializer = init)\n",
    "\n",
    "ret_old1 = model_old1(arr)\n",
    "ret_new1 = model_new1(arr)\n",
    "\n",
    "print('Old result:')\n",
    "print(ret_old1)\n",
    "print('New result:')\n",
    "print(ret_new1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A thing to note however, is that v2.0 returns a Node, which is an instance of the NumPy Ndarray class, whereas v3.0 returns a learnable_graph_element, which is a class __not__ inheriting NumPy's ndarray. This means that you should no longer call NumPy functions directly without explicitly turning the returned value into an ndarray using __as_ndarray__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.0 return value is NumPy array: True\n",
      "v3.0 return value is NumPy array: False\n",
      "\n",
      "Result from NumPy operation on v2.0 returned value:\n",
      "[ 6.  6.  6.]\n",
      "Result from NumPy operation on v3.0 returned value:\n",
      "[[ 1.  1.  1.]\n",
      " [ 5.  5.  5.]]\n",
      "Result from NumPy operation on v3.0 converted value:\n",
      "[ 6.  6.  6.]\n"
     ]
    }
   ],
   "source": [
    "print('v2.0 return value is NumPy array:',isinstance(ret_old1, np.ndarray))\n",
    "print('v3.0 return value is NumPy array:',isinstance(ret_new1, np.ndarray))\n",
    "\n",
    "print()\n",
    "print('Result from NumPy operation on v2.0 returned value:')\n",
    "print(np.sum(ret_old1, axis=0))\n",
    "print('Result from NumPy operation on v3.0 returned value:')\n",
    "print(np.sum(ret_new1, axis=0))\n",
    "print('Result from NumPy operation on v3.0 converted value:')\n",
    "print(np.sum(ret_new1.as_ndarray(), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we can chain several models together to produce networks capable of training on more complicated data sets than what a single layer could hope to accomplish. This is done in the same way as before, where the output of one operation is fed to the input of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2.0 result:\n",
      "[[  3.   3.   3.   3.   3.]\n",
      " [ 15.  15.  15.  15.  15.]]\n",
      "v3.0 result:\n",
      "[[  3.   3.   3.   3.   3.]\n",
      " [ 15.  15.  15.  15.  15.]]\n"
     ]
    }
   ],
   "source": [
    "model_old2 = rm.Dense(5, initializer = init)\n",
    "model_new2 = rm.graph.DenseGraphElement(5, initializer = init)\n",
    "\n",
    "ret_old2 = model_old2(ret_old1)\n",
    "ret_new2 = model_new2(ret_new1)\n",
    "\n",
    "print('v2.0 result:')\n",
    "print(ret_old2)\n",
    "print('v3.0 result:')\n",
    "print(ret_new2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the major differences in how v2.0 and v3.0 is used comes as a result of the new method of figuring out how to perform the backwards calculations. As explained before, v2.0 uses the history that was developed going forward, by following it backwards and picking up the gradients for each _significant_ variable.\n",
    "\n",
    "These gradients are determined by and stored in a singular __Grads__ object, which the user constructs by calling the method __grad__ on a Node. It then follows the history that is recorded in each Node backwards. In v2.0, the history is not recorded unless we explicitly tell the model that it should record the history by the __train__ context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient found through old method is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reshape([[ 5.,  5.,  5.],\n",
       "         [ 5.,  5.,  5.]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with model_old1.train():\n",
    "    ret_old3 = model_old1(arr)\n",
    "    ret_old4 = rm.sum(model_old2(ret_old3))\n",
    "print('Gradient found through old method is:')\n",
    "ret_old4.grad().get(ret_old3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In v3.0, we now construct the backward graph _while_ we are constructing the forward calculation graph, meaning that the grad object that we previously used is no longer required. Since we are no longer dealing with a history, but rather a full tree from the first calculated value to the gradient of its input, we instead perform the backward operation going forward from the current point in the graph.\n",
    "\n",
    "This is done by calling __backward__ on any element returned from a GraphFactory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient found through new method is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[ 5.  5.  5.]\n",
       " [ 5.  5.  5.]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret_new3 = model_new1(arr)\n",
    "ret_new4 = model_new2(ret_new3)\n",
    "print('Gradient found through new method is:')\n",
    "ret_new4.backward().get_gradient(ret_new3.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, in order to update the networks, we need to include an optimizer in our model. In v2.0, the optimizer is another example of a seperate utlity. Putting it simply, this old optimizer accepts gradients, and simply transforms this gradient according to the method chosen, allowing the __grad__ object from before to use this before applying updates to the graph.\n",
    "\n",
    "First we construct our optimizer object and then we feed said optimizer object to network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_old = rm.Sgd()\n",
    "ret_old4.grad().update(opt_old)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In v3.0, the optimizer is no longer a seperate process but is integrated into the calculation graph itself. This change is subtle and the way the user interacts with the update functionality has not changed, but provides more control on the back-end of things. The way to update the new graph is done as so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_new = rm.graph.sgd_update()\n",
    "ret_new4.backward().update(opt_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding data\n",
    "\n",
    "So far, we've seen how to perform the basic operations that we are used to. It is possible to use the same way of feeding data to the model using ReNom's __NdarrayDistributor__, but later we show examples on how to use the new __DistributorElement__.\n",
    "\n",
    "The current method for feeding data to ReNom, is to make use of one of the classes in the __Distributor__ module, by calling the __batch__ method, iterating over the input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]] [[0]]\n",
      "[[1]] [[2]]\n",
      "[[2]] [[4]]\n",
      "[[3]] [[6]]\n",
      "[[4]] [[8]]\n",
      "[[5]] [[10]]\n",
      "[[6]] [[12]]\n",
      "[[7]] [[14]]\n",
      "[[8]] [[16]]\n",
      "[[9]] [[18]]\n"
     ]
    }
   ],
   "source": [
    "x = np.arange(10).reshape(-1,1)\n",
    "y = x.copy() * 2\n",
    "\n",
    "distributor_old = rm.NdarrayDistributor(x,y)\n",
    "for batch, label in distributor_old.batch(batch_size = 1, shuffle = True):\n",
    "    print(batch, label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py 3.5",
   "language": "python",
   "name": "py3.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
