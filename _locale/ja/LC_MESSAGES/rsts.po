# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2017, grid
# This file is distributed under the same license as the ReNom package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2017.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: ReNom 2.0\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2017-07-07 10:53+0900\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.3.4\n"

#: ../../rsts/index.rst:7
msgid "ReNom version2.0"
msgstr ""

#: ../../rsts/installation.rst:3
msgid "Installation"
msgstr ""

#: ../../rsts/modules.rst:2
msgid "ReNomAd"
msgstr ""

#: ../../rsts/renom.rst:2
msgid "APIs"
msgstr ""

#: of renom.core.Grads:1
msgid "This class contains gradients of each Node object."
msgstr ""

#: of renom.core.Grads:3
msgid ""
"When the function grad witch is instance of Node class called, an "
"instance of Grads."
msgstr ""

#: of renom.core.Grads:8 renom.core.Node:5 renom.core.Variable:11
#: renom.layers.activation.leaky_relu.LeakyRelu:13
#: renom.layers.activation.relu.Relu:9
#: renom.layers.activation.sigmoid.Sigmoid:9
#: renom.layers.activation.tanh.Tanh:9
#: renom.layers.function.batch_normalize.BatchNormalize:14
#: renom.layers.function.conv2d.Conv2d:18 renom.layers.function.dense.Dense:9
#: renom.layers.function.dropout.Dropout:11
#: renom.layers.function.dropout.SpatialDropout:10
#: renom.layers.function.flatten.Flatten:5 renom.layers.function.lstm.Lstm:33
#: renom.layers.function.pool2d.AveragePool2d:12
#: renom.layers.function.pool2d.MaxPool2d:12 renom.operation.concat:11
#: renom.operation.dot:10 renom.operation.reshape:10 renom.operation.sqrt:9
#: renom.operation.sum:10 renom.operation.where:13 renom.optimizer.Sgd:9
msgid "Example"
msgstr ""

#: of renom.core.Grads.get:4
msgid "This method returns gradient of passed node object."
msgstr ""

#: of renom.core.Grads.update:1
msgid "Updates variables usinig gradients."
msgstr ""

#: of renom.core.Grads.update:3
msgid ""
"If an optimizer instance is passed, gradients are rescaled with regard to"
" the optimization algorithm before updating."
msgstr ""

#: of renom.core.Node:1
msgid "Bases: :class:`numpy.ndarray`"
msgstr ""

#: of renom.core.Node:1
msgid ""
"This is the base class of all operation function. Node class inherits "
"numpy ndarray class."
msgstr ""

#: of renom.core.Node.grad:1
msgid "initial: detach_graph:"
msgstr ""

#: of renom.core.Node.release_gpu:1
msgid "This method releases memory on GPU."
msgstr ""

#: of renom.core.Node.to_cpu:1
msgid "Send data witch is on the memory of GPU device to host CPU."
msgstr ""

#: of renom.core.Node.to_gpu:1
msgid "Send data witch is on the memory of host CPU to GPU device memory."
msgstr ""

#: of renom.core.Variable:1 renom.operation.reshape:1 renom.operation.sum:1
#: renom.operation.where:1
msgid "Bases: :class:`renom.core.Node`"
msgstr ""

#: of renom.core.Variable:1
msgid "Variable class."
msgstr ""

#: of renom.core.Variable:3
msgid "The gradient of this object will be calculated."
msgstr ""

#: of renom.core.Variable:5
#: renom.layers.loss.mean_squared_error.MeanSquaredError:8
#: renom.layers.loss.sigmoid_cross_entropy.SigmoidCrossEntropy:5
#: renom.layers.loss.softmax_cross_entropy.SoftmaxCrossEntropy:5
#: renom.operation.concat:5 renom.operation.concat:7 renom.operation.dot:4
#: renom.operation.dot:6 renom.operation.reshape:4 renom.operation.sqrt:5
#: renom.operation.sum:4 renom.operation.where:7 renom.operation.where:9
msgid "Input array."
msgstr ""

#: of renom.core.Variable:7
msgid "Auto update flug."
msgstr ""

#: of renom.operation.concat:1 renom.operation.dot:1
msgid "Bases: :class:`renom.core.BinOp`"
msgstr ""

#: of renom.operation.concat:1 renom.operation.sqrt:1 renom.operation.where:1
msgid ""
"This function executes concatenation of two matrixes. See numpy.concat "
"document. This function provides concatenation along the 2nd axis."
msgstr ""

#: of renom.operation.dot:1
msgid ""
"This function executes dot product of the two matrixes. See numpy.dot "
"document."
msgstr ""

#: of renom.operation.reshape:1
msgid "This function reshapes matrix shape. See numpy.reshape document."
msgstr ""

#: of renom.operation.reshape:6
msgid "Shape."
msgstr ""

#: of renom.operation.sqrt:1
msgid "Bases: :class:`renom.core.UnaryOp`"
msgstr ""

#: of renom.operation.sum:1
msgid "This function sums up matrix elements. See numpy.sum document."
msgstr ""

#: of renom.operation.sum:6
msgid "Axis of sum up.."
msgstr ""

#: of renom.operation.where:5
msgid "Condition array."
msgstr ""

#: ../../rsts/renom.cuda.rst:2
msgid "renom.cuda package"
msgstr ""

#: ../../rsts/renom.cuda.rst:5
msgid "Submodules"
msgstr ""

#: ../../rsts/renom.cuda.rst:8 ../../rsts/renom.cuda.rst:16
msgid "renom.cuda.cublas module"
msgstr ""

#: ../../rsts/renom.cuda.rst:24
msgid "renom.cuda.cuda module"
msgstr ""

#: ../../rsts/renom.cuda.rst:32
msgid "renom.cuda.cuda_base module"
msgstr ""

#: ../../docstring of renom.cuda.cuda_base.Mempool:1
#: renom.cuda.cuda_base.gpu_allocator_with_mem_pool:1
#: renom.cuda.cudnn.TensorDesc:1
msgid "Bases: :class:`object`"
msgstr ""

#: ../../rsts/renom.cuda.rst:40 ../../rsts/renom.cuda.rst:48
msgid "renom.cuda.cudnn module"
msgstr ""

#: ../../rsts/renom.cuda.rst:56
msgid "renom.cuda.thrust module"
msgstr ""

#: ../../rsts/renom.cuda.rst:64
msgid "renom.cuda.thrust_double module"
msgstr ""

#: ../../rsts/renom.cuda.rst:72
msgid "renom.cuda.thrust_float module"
msgstr ""

#: ../../rsts/renom.cuda.rst:81
msgid "Module contents"
msgstr ""

#: ../../rsts/renom.layers.rst:2
msgid "renom.layers package"
msgstr ""

#: ../../rsts/renom.layers.activation.rst:2
msgid "renom.layers.activation"
msgstr ""

#: of renom.layers.activation.leaky_relu.LeakyRelu:1
msgid ""
"The Leaky relu [1]_ activation function is described by the following "
"formula:"
msgstr ""

#: of renom.layers.activation.leaky_relu.LeakyRelu:3
msgid ":math:`f(x)=max(x, 0)+min(slope*x, 0)`"
msgstr ""

#: of renom.layers.activation.leaky_relu.LeakyRelu:7
msgid "Input numpy array or instance of Variable."
msgstr ""

#: of renom.layers.activation.leaky_relu.LeakyRelu:9
msgid "Coefficient multiplied by negative values."
msgstr ""

#: of renom.layers.activation.leaky_relu.LeakyRelu:26
msgid ""
"Andrew L. Maas, Awni Y. Hannun, Andrew Y. Ng (2014). Rectifier "
"Nonlinearities Improve Neural Network Acoustic Models"
msgstr ""

#: of renom.layers.activation.relu.Relu:1
msgid ""
"Rectified Linear Unit activation function as described by the following "
"formula."
msgstr ""

#: of renom.layers.activation.relu.Relu:3
msgid ":math:`f(x)=max(x, 0)`"
msgstr ""

#: of renom.layers.activation.relu.Relu:5
#: renom.layers.activation.sigmoid.Sigmoid:5
#: renom.layers.activation.tanh.Tanh:5
msgid "Input numpy array or Node instance."
msgstr ""

#: of renom.layers.activation.sigmoid.Sigmoid:1
msgid "Sigmoid activation function as described by the following formula."
msgstr ""

#: of renom.layers.activation.sigmoid.Sigmoid:3
msgid ":math:`f(x) = 1/(1 + \\exp(-x))`"
msgstr ""

#: of renom.layers.activation.tanh.Tanh:1
msgid ""
"Hyperbolic tangent activation function as described by the following "
"formula."
msgstr ""

#: of renom.layers.activation.tanh.Tanh:3
msgid ":math:`f(x) = tanh(x)`"
msgstr ""

#: ../../rsts/renom.layers.function.rst:2
msgid "renom.layers.function"
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:1
msgid "Batch normalization function [1]_."
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:3
msgid ""
"If the argument mode is set to 'feature', normalize prior-layer features "
"per patch."
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:6
msgid "Momentum coefficient for the moving average."
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:8
msgid "'activation'  or 'feature'"
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:10
msgid "Small number added to avoid division by zero"
msgstr ""

#: of renom.layers.function.batch_normalize.BatchNormalize:26
msgid ""
"Sergey Ioffe, Christian Szegedy.(2015).Batch Normalization: Accelerating "
"Deep Network Training by Reducing Internal Covariate Shift"
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:1
msgid "2d convolution function."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:3
msgid ""
"This class creates a convolution filter to be convolved with the input "
"tensor. The instance of this class only accepts and outputs 4d tensors. "
"In the case of int input, filter, padding, and stride, the shape will be "
"symmetric."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:8
msgid "The dimensionality of the output."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:10
#: renom.layers.function.pool2d.AveragePool2d:4
#: renom.layers.function.pool2d.MaxPool2d:4
msgid "Filter size of the convolution kernel."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:12
#: renom.layers.function.pool2d.AveragePool2d:6
#: renom.layers.function.pool2d.MaxPool2d:6
msgid "Size of the zero-padding around the image."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:14
#: renom.layers.function.pool2d.AveragePool2d:8
#: renom.layers.function.pool2d.MaxPool2d:8
msgid "Stride-size of the convolution."
msgstr ""

#: of renom.layers.function.conv2d.Conv2d:30
msgid "Tensor data format is **NCHW**."
msgstr ""

#: of renom.layers.function.dense.Dense:1
msgid "Fully connected layer."
msgstr ""

#: of renom.layers.function.dense.Dense:3
msgid ":math:`f(x)= w \\cdot x + b`"
msgstr ""

#: of renom.layers.function.dense.Dense:5
msgid "Output unit size."
msgstr ""

#: of renom.layers.function.dropout.Dropout:1
msgid "Applies Dropout [2]_ to the input."
msgstr ""

#: of renom.layers.function.dropout.Dropout:3
msgid ""
"Dropout function randomly selects a fraction (specified by dropout_ratio)"
" of the data sets them to zero. Remaining data will be rescaled by ``1/(1"
" - dropout_ratio)``."
msgstr ""

#: of renom.layers.function.dropout.Dropout:7
#: renom.layers.function.dropout.SpatialDropout:4
msgid "Dropout ratio."
msgstr ""

#: of renom.layers.function.dropout.Dropout:33
msgid ""
"Hinton, Geoffrey E.; Srivastava, Nitish; Krizhevsky, Alex; Sutskever, "
"Ilya; Salakhutdinov, Ruslan R. (2012). Improving neural networks by "
"preventing co-adaptation of feature detectors"
msgstr ""

#: of renom.layers.function.dropout.SpatialDropout:1
msgid ""
"Applies spatial dropout to the input. This function drops feature maps "
"randomly."
msgstr ""

#: of renom.layers.function.dropout.SpatialDropout:7
msgid ""
":exc:`AssertionError` -- An assertion error will be raised if the input "
"tensor dimension is not 4."
msgstr ""

#: of renom.layers.function.dropout.SpatialDropout:35
msgid "SpatialDropout accepts only 4d tensor data."
msgstr ""

#: of renom.layers.function.flatten.Flatten:1
msgid "This function flattens an input tensor. It does not affect the batch size."
msgstr ""

#: of renom.layers.function.lrn.Lrn:1
msgid "Local response normalization function [3]_ ."
msgstr ""

#: of renom.layers.function.lrn.Lrn:6
msgid ""
":math:`x_{c,i,j}` represents the c th conv. filter’s output at the "
"position of (i, j) in the feature map. :math:`y_{c_{out},j,i}` represents"
" the output of local response normalization. :math:`N` is the number of "
"the feature maps. :math:`n` is the adjacent feature map number. The "
"default argument values are recommended."
msgstr ""

#: of renom.layers.function.lrn.Lrn:12
msgid "Number of images used to be normalized."
msgstr ""

#: of renom.layers.function.lrn.Lrn:14
msgid "Constant."
msgstr ""

#: of renom.layers.function.lrn.Lrn:16
msgid "Scale factor."
msgstr ""

#: of renom.layers.function.lrn.Lrn:18
msgid "Exponential factor."
msgstr ""

#: of renom.layers.function.lrn.Lrn:21
msgid ""
"Alex Krizhevsky Krizhevsky, , Ilya IlyaSutskever Sutskever, Geoff. "
"ImageNet Classification with Deep Convolutional Neural Networks"
msgstr ""

#: of renom.layers.function.lstm.Lstm:1
msgid ""
"Long short time memory with peephole [4]_ . Lstm object has 12 weights "
"and 4 biases parameters to learn."
msgstr ""

#: of renom.layers.function.lstm.Lstm:4
msgid ""
"Weights applied to the input of the input gate, forget gate and output "
"gate. :math:`W_{ij}, Wgi_{ij}, Wgf_{ij}, Wgo_{ij}`"
msgstr ""

#: of renom.layers.function.lstm.Lstm:7
msgid ""
"Weights applied to the recuurent input of the input gate, forget gate and"
" output gate. :math:`R_{ij}, Rgi_{ij}, Rgf_{ij}, Rgo_{ij}`"
msgstr ""

#: of renom.layers.function.lstm.Lstm:10
msgid ""
"Weights applied to the state input of the input gate, forget gate and "
"output gate. :math:`P_{ij}, Pgi_{ij}, Pgf_{ij}, Pgo_{ij}`"
msgstr ""

#: of renom.layers.function.lstm.Lstm:50
msgid "Learning Precise Timing with LSTM Recurrent Networks"
msgstr ""

#: of renom.layers.function.lstm.Lstm.truncate:1
msgid "Truncates temporal connection."
msgstr ""

#: of renom.layers.function.pool2d.AveragePool2d:1
msgid ""
"Average pooling function. In the case of int input, filter, padding, and "
"stride, the shape will be symmetric."
msgstr ""

#: of renom.layers.function.pool2d.MaxPool2d:1
msgid ""
"Max pooling function. In the case of int input, filter, padding, and "
"stride, the shape will be symmetric."
msgstr ""

#: ../../rsts/renom.layers.loss.rst:2
msgid "renom.layers.loss package"
msgstr ""

#: of renom.layers.loss.cliped_mean_squared_error.ClipedMeanSquaredError:1
msgid ""
"Cliped mean squared error function. In the forward propagation, this "
"function yields same calculation as mean squared error."
msgstr ""

#: of renom.layers.loss.cliped_mean_squared_error.ClipedMeanSquaredError:5
msgid "In the backward propagation, this function calculates following formula."
msgstr ""

#: of renom.layers.loss.cliped_mean_squared_error.ClipedMeanSquaredError:12
msgid "Input data."
msgstr ""

#: of renom.layers.loss.cliped_mean_squared_error.ClipedMeanSquaredError:14
msgid "Target data."
msgstr ""

#: of renom.layers.loss.cliped_mean_squared_error.ClipedMeanSquaredError:16
msgid "Clipping threshold."
msgstr ""

#: of renom.layers.loss.mean_squared_error.MeanSquaredError:1
msgid "This function calcurates mean squared error between x and y."
msgstr ""

#: of renom.layers.loss.mean_squared_error.MeanSquaredError:6
msgid ":math:`N` is batch size."
msgstr ""

#: of renom.layers.loss.mean_squared_error.MeanSquaredError:10
#: renom.layers.loss.sigmoid_cross_entropy.SigmoidCrossEntropy:7
#: renom.layers.loss.softmax_cross_entropy.SoftmaxCrossEntropy:7
msgid "Target array."
msgstr ""

#: ../../rsts/renom.optimizer.rst:2
msgid "renom.optimizer"
msgstr ""

#: of renom.optimizer.Adagrad:1 renom.optimizer.Adam:1
#: renom.optimizer.Rmsprop:1 renom.optimizer.Sgd:1
msgid "Bases: :class:`renom.optimizer.Optimizer`"
msgstr ""

#: of renom.optimizer.Adagrad:1 renom.optimizer.Adam:1
#: renom.optimizer.Rmsprop:1 renom.optimizer.Sgd:3
msgid "Learning rate."
msgstr ""

#: of renom.optimizer.Sgd:1
msgid "Stochastic Gradient Descent."
msgstr ""

#: of renom.optimizer.Sgd:5
msgid "Momentum coefficient of optimization."
msgstr ""

#: ../../rsts/tutorial0-AutomaticDifferentiation.ipynb:6
msgid "Tutorial0 Auto Differentiation"
msgstr ""

#: ../../rsts/tutorial0-AutomaticDifferentiation.ipynb:24
msgid "Data preparation"
msgstr ""

#: ../../rsts/tutorial0-AutomaticDifferentiation.ipynb:64
#: ../../rsts/tutorial2-cifar10.ipynb:127
msgid "Neural network definition"
msgstr ""

#: ../../rsts/tutorial0-AutomaticDifferentiation.ipynb:98
#: ../../rsts/tutorial1-mnist.ipynb:238 ../../rsts/tutorial2-cifar10.ipynb:221
msgid "Training loop"
msgstr ""

#: ../../rsts/tutorial0-AutomaticDifferentiation.ipynb:247
msgid "Prediction"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:6
msgid "Tutorial 1 Mnist classifier"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:14 ../../rsts/tutorial2-cifar10.ipynb:18
msgid "Required libraries"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:67 ../../rsts/tutorial2-cifar10.ipynb:47
msgid "GPU-enabled Computing"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:69 ../../rsts/tutorial2-cifar10.ipynb:49
msgid ""
"If you wish to use a GPU, you'll need to call the ``set_cuda_active()`` "
"with the single argument ``True``. This will generally allow training to "
"run much faster than relying on the CPU. You'll need an nVidia GPU "
"installed on your machine."
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:88 ../../rsts/tutorial2-cifar10.ipynb:68
msgid "Load data"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:90
msgid ""
"Next, we have to load-in the raw, binary MNIST data and shape into "
"training-ready objects. To accomplish this, we'll use the "
"``fetch_mldata`` module included in the scikit-learn package."
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:94
msgid ""
"The MNIST dataset consists of 70000 digit images. Before we do anything "
"else, we have to split the data into a training set and a test set. We'll"
" then do two important pre-processing steps that make for a smoother "
"training process: **1)** Re-scale the image data (originaly integer "
"values 0-255) to have a range from 0 to 1. **2)**''Binarize'' the labels-"
" map each digit (0-9) to a vector of 0s and 1s."
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:131
msgid "Define the neural network"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:133
msgid ""
"Next, we setup the neural network itself. In this tutorial, we'll "
"construct a 2-layer network. In other words, it will consist of an input "
"layer in additional to 2 parametrized layers. We then have to choose an "
"activation function for the neurons. Although the ''sigmoid'' function is"
" a popular starting example in ML courses, the ''rectified linear "
"function'' (ReLu) is becoming a popular default when training neural "
"networks. We'll use the ReLU function here."
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:141
msgid ""
"For validation and prediction purposes, it's not necessary build a "
"computational graph. During these two processes, we'll disable the graph "
"in order to conserve memory usage,. This is done by calling the method "
"``detach_graph()``"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:170
msgid "Neural network with a L2 normalization term"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:199 ../../rsts/tutorial2-cifar10.ipynb:168
msgid "Definition of a nural network with sequential model"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:219 ../../rsts/tutorial2-cifar10.ipynb:202
msgid "Instantiation"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:240
msgid ""
"Now that the network is built, we can start to do the actual training. "
"Rather than using vanilla \"batch\" gradient descent, which is "
"computationally expensive, we'll use mini-batch stochastic gradient "
"descent (SGD). This method trains on a handful of examples per iteration "
"(the \"batch-size\"), allowing us to make \"stochastic\" updates to the "
"weights in a short time. The learning curve will appear noisier, but this"
" method tends to converge much faster."
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:342 ../../rsts/tutorial2-cifar10.ipynb:323
msgid "Model evaluation"
msgstr ""

#: ../../rsts/tutorial1-mnist.ipynb:344
msgid ""
"After training our model, we have to evaluate it. For each class (digit),"
" we'll use several scoring metrics: precision, recall, F1 score, and "
"support, to get a full sense of how the model performs on our test data."
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:6
msgid "Tutorial 2 Cifar10 Image classifier"
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:8
msgid ""
"In this tutorial, we'll apply a convolutional neural network (CNN) to "
"another standardized dataset, the \"CIFAR\" data. This dataset is a "
"collection of 60,000 images over 10 classes."
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:70
msgid ""
"Here we just unpickle the CIFAR image data, collected from the CIFAR "
"website (https://www.cs.toronto.edu/~kriz/cifar.html). As in Tutorial 1, "
"we scale the data to a range of 0 to 1 and binarize the labels."
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:129
msgid ""
"Setup the CNN- essentially similar to Tutorial 1, except that here we are"
" using several hidden layers. Also, we try to avoid over-fitting, by "
"using the''dropout'' technique."
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:223
msgid ""
"If you are using a GPU, we strongly recommend running a validation step "
"for each minibatch. This will improve the training performance, and "
"prevent overfitting. This also allows you to diagnose training problems "
"by comparing the validation and training learning curves."
msgstr ""

#: ../../rsts/tutorial2-cifar10.ipynb:325
msgid ""
"Finally, we evaluate the models labeling performance using the same "
"metrics as in Tutorial 1."
msgstr ""

#: ../../rsts/tutorials.rst:2
msgid "Tutorials"
msgstr ""

